# Currently tested with Kubernetes 1.10 on Minkube
common:

  #Disable sharding in the ES cluster during helm upgrade procedure and enabling afterwards.
  disable_sharding: false
  #Enable this flag if you want to upgrade ES cluster version according to https://www.elastic.co/guide/en/elasticsearch/reference/7.10/rolling-upgrades.html#rolling-upgrades
  es_upgrade_order: false
  # Elasticsearch cluster name, maps to cluster.name
  cluster_name: "searchguard"
  #Set to true of you want to remove persistent storage claims with helm delete command
  remove_pvc_on_delete: true
  #Docker images used in Kubernetes cluster. Default ones are OSS images provided by Search Guard
  #If you use custom images for ES, Kibana and sg-admin insatnces, please, use these naming convention for them:
  # - for SG Admin image: <images.provider>/<images.sgadmin_base_image>:<elkversion>-<sgversion>
  # - for Elasticsearch OSS image: <images.provider>/<images.elasticsearch_base_image>:<elkversion>-oss-<sgversion>
  # - for Elasticsearch non-OSS image: <images.provider>/<images.elasticsearch_base_image>:<elkversion>-<sgversion>
  # - for Kibana OSS image: <images.provider>/<images.kibana_base_image>:<elkversion>-oss-<sgversion>
  # - for Kibana non-OSS image: <images.provider>/<images.kibana_base_image>:<elkversion>-<sgversion>
  images:
    provider: "floragunncom"
    elasticsearch_base_image: "sg-elasticsearch"
    kibana_base_image: "sg-kibana"
    sgadmin_base_image: "sg-sgadmin"

  #Fill in to authenticate to private docker registry
  docker_registry:
    enabled: false
  #    server: https://index.docker.io/v1/
  #    username: username
  #    password: password
  #    email: info@domain.com

  # Elasticsearch and Kibana version
  # See whats available: https://hub.docker.com/r/floragunncom/sg-elasticsearch/tags
  elkversion: "7.9.2"


  # Search Guard plugin version
  sgversion: "46.0.0"

  # Search Guard Kibana plugin version
  # See whats available: https://hub.docker.com/r/floragunncom/sg-kibana/tags
  sgkibanaversion: "46.0.0"

  # If true then install also all free and basic x-pack features
  # If false then only the "oss" version of Elasticsearch and Kibana gets installed 
  xpack_basic: false

  # DN of the admin certificate
  # See https://docs.search-guard.com/latest/sgadmin#configuring-the-admin-certificate
  admin_dn:
    - "CN=sgadmin,OU=Ops,O=Example Com\\, Inc.,DC=example,DC=com"

  # Search Guard needs to securely and reliably identify internal communication between 
  # Elasticsearch nodes (inter-node traffic). This communication happens for example if 
  # one node receives a GET request on the HTTP layer, but needs to forward it to another 
  # node that holds the actual data.
  # See https://docs.search-guard.com/latest/tls-in-production#node-certificates
  nodes_dn:
    - "CN=*-esnode,OU=Ops,O=Example Com\\, Inc.,DC=example,DC=com"

  # Enable or disable Search Guard enterprise modules
  # If you run Search Guard in production you need to obtain a license
  #  when set to 'true'
  sg_enterprise_modules_enabled: true
  
  # Run automatically sgadmin whenever neccessary 
  update_sgconfig_on_change: true

  # Restart podsautomatically when their configuration was changed
  restart_pods_on_config_change: true
  #Enable Pod Disruption budget feature for ES and Kibana pods. See more https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  pod_disruption_budget_enable: false

  # Search Guard can be run do not fail on forbidden mode.
  # With this mode enabled Search Guard filters all indices from a query a user does not have access to. 
  # Thus not security exception is raised.
  # See https://docs.search-guard.com/latest/kibana-plugin-installation#configuring-elasticsearch-enable-do-not-fail-on-forbidden
  do_not_fail_on_forbidden: false
  #After obtaining a Search Guard license, you can add it here. If you don't have a license, please, specify "none"
  license: none
  #By default the jobs that generated certificate are removed after completion by cleanup jobs starting every minute.
  #By enabling debug_job_mode you suspend cleanup jobs and investigate the results of the sgadmin-preinstall and sgadmin-postinstall jobs
  debug_job_mode: false

  # Defines the service type for all elasticsearch outward-facing (non-discovery) services.
  # This does not apply to Kibana.
  # WARNING: Setting this to 'LoadBalancer' will probably expose elasticsearch to the outside/internet
  serviceType: ClusterIP
  #serviceType: NodePort
  #serviceType: LoadBalancer

  #In case IngressNginx is enabled in the cluster, Kibana and Elasticsearch services are available in the cluster with specified domain names ingressKibanaDomain and ingressElasticsearchDomain
  #The certificate are generated according to PKI approach chosen in the cluster:
  # - sgadmin_certificates_enabled - for self-signed certificate generated by SG TLS tool
  # - ca_certificates_enabled - for uploading CA certificate and service certificate with it
  # - external_ca_certificates_enabled - for uploading elasticsearch.pem, elasticsearch.key, kibana.pem, kibana.key from ../keys/nodes folder and use them for services
  ingressNginx:
    enabled: true
    #For ingressCertificates use "self-signed" if you want auto-generated with TLS tool self-signed certificates
    #For ingressCertificates use "external" if you have copied trusted certificates with the names tls.key and tls.pem to folders ../keys/ingress_certificates/elasticsearch/ and  ../keys/ingress_certificates/kibana/
    ingressCertificates: "self-signed"
    #ingressCertificates: "external"
    #ingressCertificates: "none"
    ingressKibanaDomain: "kibana.sg-helm.example.com"
    #ingressKibanaDomain: "none"
    ingressElasticsearchDomain: "es.sg-helm.example.com"
    #ingressElasticsearchDomain: "none"

  #PKI aprroaches are specified with the following keys:
  #Set sgadmin_certificates_enabled if certificates are self-signed and generated by SG TLS tool
  sgadmin_certificates_enabled: true
  #Set ca_certificates_enabled to true if CA cert and key are copied to ../keys/ca/ folder and used by cluster to generate all certificates for nodes and services
  ca_certificates_enabled: false
  #Set external_ca_certificates_enabled to true if add all certificates for nodes and services to the folder ../keys/nodes/ to use them for encryption
  external_ca_certificates_enabled: false
  #Set external_ca_certificates_enabled to true if add single certificates for all nodes and services to the folder ../keys/sg-esnode/ to use it for encryption
  external_ca_single_certificate_enabled: false
  #Set directory where certificates are stored
  certificates_directory: "secrets"


  # Any extra or specific configuration that is needed can be added here.
  # Will be added to all elasticsearch.yml files on all nodes


#  config:
#    http:
#      compression: false
#      cors:
#        enabled: false
#        allow-origin: "*"
#    index.codec: best_compression


#  # Configure additional users (maps to sg_internal_users.yml)
#  users:
#    demouser:
#      hash: ${envbc.SG_DEMOUSER_PWD}
#      backend_roles:
#        - beatsreader
#
#
#  # Configure additional rolemappings (maps to sg_roles_mapping.yml)
#  rolesmapping:
#    sg_read_beats:
#      backend_roles:
#        - beatsreader
#
#  # Configure additional role (maps to sg_roles.yml)
#  roles:
#    sg_read_beats:
#      cluster_permissions:
#        - SGS_CLUSTER_COMPOSITE_OPS_RO
#      index_permissions:
#        - index_patterns:
#            - "*beat*"
#          allowed_actions:
#            - SGS_READ

  # If you want any plugins installed, give them here as a list. They will be
  # passed to elasticsearch-plugin install -b {line here}
  # Do not add the searchguard plugin here, because its already installed in the main image
  #plugins:
  #  - analysis-phonetic

# Client/ingest nodes can execute pre-processing pipelines, composed of
# one or more ingest processors. Depending on the type of operations performed
# by the ingest processors and the required resources, it may make sense to
# have dedicated ingest nodes, that will only perform this specific task.
client:
  # For production we recommend at least 2 replicas
  replicas: 1
  # 'hard' means that pods will only be scheduled if there are enough nodes for them and that they will never end up on the same node. 
  # 'soft' will do this "best effort"
  # For production we recommend setting this to 'hard'
  antiAffinity: "soft"
  # Specify this if external_ca_certificates_enabled: true
  storage: 2Gi
  storageClass: "standard"

  # For production we recommend to set this to at least 4g and adjust the memory limits and requests accordingly
  heapSize: 1g
  # More info on what this setting does is in the config map. Only change this
  # if you set the cpu limit to over 1 full cpu.
  processors: 1
  labels: {}
  annotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 1500Mi
    requests:
      cpu: 100m
      memory: 1500Mi

# Data nodes hold the shards that contain the documents you have indexed. Data
# nodes handle data related operations like CRUD, search, and aggregations.
# These operations are I/O-, memory-, and CPU-intensive. It is important to
# monitor these resources and to add more data nodes if they are overloaded.
#
# The main benefit of having dedicated data nodes is the separation of the
# master and data roles.
data:
  # For production we recommend at least 2 replicas
  replicas: 1
  # 'hard' means that pods will only be scheduled if there are enough nodes for them and that they will never end up on the same node. 
  # 'soft' will do this "best effort"
  # For production we recommend setting this to 'hard'
  antiAffinity: "soft"
  storage: 4Gi
  storageClass: "standard"
  # For production we recommend to set this to at least 8g and adjust the memory limits and requests accordingly
  heapSize: 1g
  # More info on what this setting does is in the config map. Only change this
  # if you set the cpu limit to over 1 full cpu.
  processors: 1
  labels: {}
  annotations: {}
  resources:
    limits:
      cpu: 1
      memory: 1500Mi
    requests:
      cpu: 1
      memory: 1500Mi

# The master node is responsible for lightweight cluster-wide actions such as
# creating or deleting an index, tracking which nodes are part of the
# cluster, and deciding which shards to allocate to which nodes. It is
# important for cluster health to have a stable master node.
master:
  # For production we recommend at least 3 replicas. Number must be odd.
  replicas: 1
  # 'hard' means that pods will only be scheduled if there are enough nodes for them and that they will never end up on the same node. 
  # 'soft' will do this "best effort"
  # For production we recommend setting this to 'hard'
  antiAffinity: "soft"
  storage: 2Gi
  storageClass: "standard"
  # For production we recommend to set this to at least 2g and adjust the memory limits and requests accordingly
  heapSize: 1g
  # More info on what this setting does is in the config map. Only change this
  # if you set the cpu limit to over 1 full cpu.
  processors: 1
  labels: {}
  annotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 1500Mi
    requests:
      cpu: 100m
      memory: 1500Mi

kibana:
  # Incoming port of the service
  httpPort: 5601
  replicas: 1
  # 'hard' means that pods will only be scheduled if there are enough nodes for them and that they will never end up on the same node. 
  # 'soft' will do this "best effort"
  # For production we recommend setting this to 'hard'
  antiAffinity: "soft"

  # Specify this if external_ca_certificates_enabled: true
  storage: 2Gi
  storageClass: "standard"

  # Defines the service type for all Kibana outward-facing (non-discovery) services.
  # WARNING: Setting this to 'LoadBalancer' will probably expose Kibana to the outside/internet
  serviceType: ClusterIP
  #serviceType: NodePort
  #serviceType: LoadBalancer
  # Additional config which will be appended to kibana.yml
  config:
    elasticsearch.requestHeadersWhitelist: ["sgtenant","authorization"]
    # The authentication type, on of: 'basicauth', 'jwt', 'openid', 'saml', 'proxy', 'kerberos'
    searchguard.auth.type: basicauth
    # Disallow login for service users
    searchguard.basicauth.forbidden_usernames: ["kibanaserver"]
  labels: {}
  annotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 2500Mi
    requests:
      cpu: 100m
      memory: 2500Mi

# Incoming port of the service
service:
  httpPort: 9200
  transportPort: 9300

# Kubernetes Role-based access control
# https://kubernetes.io/docs/reference/access-authn-authz/rbac/
#Use "true" for first time installation for certain release in certain cluster/namespace
#Use "false" for second+ time installations for certain release in certain cluster/namespace
rbac:
  create: true


# Kubelet image pull policy
# https://kubernetes.io/docs/concepts/containers/images/
#pullPolicy: Always
pullPolicy: IfNotPresent
  
